lenet-5 구글링

퍼셉트론(perceptron)
AND, OR 연산 등 선형 분리가 가능한 문제의 해결에만 사용될 수 있슴

이미지의 특징(x)
convolution + pooling layers(image의 특징을 찾아줌)

커널이 웨이트(w) 역활을함]

편차(mse)

퍼셉트론에서 웨이트는 열백터





역전파(BackPropagation)
- 
주피터 코랩
런타임 -> 런타임 유형 변경


softmax
- 0과 1사이값으로 정규화를 해주는 함수(확률처럼 모든 아웃풋 값을 더하면 1이나온다)

loss funstion
- 손실함수

optimization
- 최적화

optimizer
- 최적화 알고리즘(adam, sgd)

kernel_regularizer
- 규제(L1,L2 규제가있음)

원-핫 인코딩(One-Hot Encoding)
- 원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고,
 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식

Adam(Adaptive Moment Estimation)
- 모멘텀과 RMSprop을 섞어놓은 최적화 알고리즘 입기 때문에, 딥러닝에서 가장 흔히 사용되는 최적화 알고리즘

SGD(확률적 경사 하강법/Stochastic Gradient Descent)
- 랜덤하게 추출한 일부 데이터를 사용하는 것이다. 따라서 학습 중간 과정에서 결과의 진폭이 크고 불안정하며,
 속도가 매우 빠르다. 또한, 데이터 하나씩 처리하기 때문에 오차율이 크고 GPU의 성능을 모두 활용하지 못하는 단점을 가진다.
 이러한 단점들을 보완하기 위해 나온 방법들이 Mini batch를 이용한 방법이며, 확률적 경사 하강법의 노이즈를 줄이면서도 전체 배치보다 더 효율적인 것

evaluate
- 최종적인 정답률과 loss를 알수 있다

Transfer learning(전이학습)
- 입력층에 가까운 부분의 결합 파라미터는 학습된 값으로 변화시키지 않음

Fine Tuning(파인 튜닝)
- 출력층 및 출력층에 가까운 부분뿐만 아니라 모든 층의 파라미터 다시 학습