lenet-5 구글링


이미지의 특징(x)
convolution + pooling layers(image의 특징을 찾아줌)

커널이 웨이트(w) 역활을함]

편차(mse)

퍼셉트론에서 웨이트는 열백터

딥러닝 순서
1. 데이터로드
2. 데이터 전처리
- 특성 공학
- 특성 선택
- 특성 추출
3. 데이터 분할
4. 모델 생성&컴파일
5. 훈련
6. 검증
7. 예측

딥러닝 실무의 흐름
-> 문제 정의하기 - 스펙 결정하기 - 실행 가능성 확인하기 - 알고리즘 설계하기 - 데이터 정리하기 - 모델 학습 및 검증하기 - 프로그램 전달하기

역전파(BackPropagation)
- 역전파는 반대로 출력층에서 입력층 방향으로 계산하면서 가중치를 업데이트

주피터 코랩
런타임 -> 런타임 유형 변경


softmax
- 0과 1사이값으로 정규화를 해주는 함수(확률처럼 모든 아웃풋 값을 더하면 1이나온다)

loss funstion
- 손실함수 0.5 * ((y_hat -y)** 2).sum()

optimization
- 최적화

optimizer
- 최적화 알고리즘(adam, sgd)

kernel_regularizer
- 규제(L1,L2 규제가있음)

원-핫 인코딩(One-Hot Encoding)
- 원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고,
 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식

레이블 인코딩 (Label Encoding)
- 레이블 인코딩은 카테고리 피처(문자열)를 코드형 숫자 값으로 변환하는 것입니다.

Adam(Adaptive Moment Estimation)
- 모멘텀과 RMSprop을 섞어놓은 최적화 알고리즘 입기 때문에, 딥러닝에서 가장 흔히 사용되는 최적화 알고리즘

SGD(확률적 경사 하강법/Stochastic Gradient Descent)
- 랜덤하게 추출한 일부 데이터를 사용하는 것이다. 따라서 학습 중간 과정에서 결과의 진폭이 크고 불안정하며,
 속도가 매우 빠르다. 또한, 데이터 하나씩 처리하기 때문에 오차율이 크고 GPU의 성능을 모두 활용하지 못하는 단점을 가진다.
 이러한 단점들을 보완하기 위해 나온 방법들이 Mini batch를 이용한 방법이며, 확률적 경사 하강법의 노이즈를 줄이면서도 전체 배치보다 더 효율적인 것

evaluate(x_test,y_test)
- 최종적인 정답률과 loss를 알수 있다

Transfer learning(전이학습)
- 입력층에 가까운 부분의 결합 파라미터는 학습된 값으로 변화시키지 않음

Fine Tuning(파인 튜닝)
- 출력층 및 출력층에 가까운 부분뿐만 아니라 모든 층의 파라미터 다시 학습

tf.constant
- 상수(값변경 불가능)

tf.Variable
- 값 변경 가능,

activation='relu'
- 0보다 작으면 0
  0보다 크면 자기자신을 출력한다

loss
: 훈련 손실값

acc 
: 훈련 정확도

val_loss
: 검증 손실값

val_acc 
: 검증 정확도
  tf.convert_to_tensor()로 상수 변경 가능

Stride
- 높을 수록 연산량이 많아짐(낮으면 촘촘하게, 높으면 넓은간격으로)

Perceptron(퍼셉트론)
- 프랑크 로젠블라트(Frank Rosenblatt)가 1957년에 제안한 초기 형태의 인공 신경망으로 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘
- AND, OR 연산 등 선형 분리가 가능한 문제의 해결에만 사용될 수 있슴


expand_dims
- 배열의 차원을 추가

flow_from_directory
- 

aitex 예제
